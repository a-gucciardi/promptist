{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a06056a",
   "metadata": {},
   "source": [
    "Test notebook - from kobold docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb96819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "user = \"User:\"\n",
    "bot = \"Bot:\"\n",
    "ENDPOINT = \"http://localhost:5001/api\"\n",
    "conversation_history = [] # using a list to update conversation history is more memory efficient than constantly updating a string\n",
    "\n",
    "def get_prompt(user_msg):\n",
    "    return {\n",
    "        \"prompt\": f\"{user_msg}\",\n",
    "        \"use_story\": \"False\", # Use the story from the KoboldAI UI, can be managed using other API calls (See /api for the documentation)\n",
    "        \"use_memory\": \"False\", # Use the memnory from the KoboldAI UI, can be managed using other API calls (See /api for the documentation)\n",
    "        \"use_authors_note\": \"False\", # Use the authors notes from the KoboldAI UI, can be managed using other API calls (See /api for the documentation)\n",
    "        \"use_world_info\": \"False\", # Use the World Info from the KoboldAI UI, can be managed using other API calls (See /api for the documentation)\n",
    "        \"max_context_length\": 2048, # How much of the prompt will we submit to the AI generator? (Prevents AI / memory overloading)\n",
    "        \"max_length\": 100, # How long should the response be?\n",
    "        \"rep_pen\": 1.1, # Prevent the AI from repeating itself\n",
    "        \"rep_pen_range\": 2048, # The range to which to apply the previous\n",
    "        \"rep_pen_slope\": 0.7, # This number determains the strength of the repetition penalty over time\n",
    "        \"temperature\": 0.5, # How random should the AI be? In a low value we pick the most probable token, high values are a dice roll\n",
    "        \"tfs\": 0.97, # Tail free sampling, https://www.trentonbricken.com/Tail-Free-Sampling/\n",
    "        \"top_a\": 0.0, # Top A sampling , https://github.com/BlinkDL/RWKV-LM/tree/4cb363e5aa31978d801a47bc89d28e927ab6912e#the-top-a-sampling-method\n",
    "        \"top_k\": 0, # Keep the X most probable tokens\n",
    "        \"top_p\": 0.9, # Top P sampling / Nucleus Sampling, https://arxiv.org/pdf/1904.09751.pdf\n",
    "        \"typical\": 1.0, # Typical Sampling, https://arxiv.org/pdf/2202.00666.pdf\n",
    "        \"sampler_order\": [6,0,1,3,4,2,5], # Order to apply the samplers, our default in this script is already the optimal one. KoboldAI Lite contains an easy list of what the\n",
    "        \"stop_sequence\": [f\"{user}\"], # When should the AI stop generating? In this example we stop when it tries to speak on behalf of the user.\n",
    "        #\"sampler_seed\": 1337, # Use specific seed for text generation? This helps with consistency across tests.\n",
    "        \"singleline\": \"False\", # Only return a response that fits on a single line, this can help with chatbots but also makes them less verbose\n",
    "        \"sampler_full_determinism\": \"False\", # Always return the same result for the same query, best used with a static seed\n",
    "        \"frmttriminc\": \"True\", # Trim incomplete sentences, prevents sentences that are unfinished but can interfere with coding and other non english sentences\n",
    "        \"frmtrmblln\": \"False\", #Remove blank lines\n",
    "        \"quiet\": \"False\" # Don't print what you are doing in the KoboldAI console, helps with user privacy\n",
    "        }\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_message = input(f\"{user} \")\n",
    "\n",
    "        if len(user_message.strip()) < 1:\n",
    "            print(f\"{bot}Please provide a valid input.\")\n",
    "            continue\n",
    "\n",
    "        fullmsg = f\"{conversation_history[-1] if conversation_history else ''}{user} {user_message}\\n{bot}\" # Add all of conversation history if it exists and add User and Bot names\n",
    "        prompt = get_prompt(fullmsg) # Process prompt into KoboldAI API format\n",
    "        response = requests.post(f\"{ENDPOINT}/v1/generate\", json=prompt) # Send prompt to API\n",
    "        if response.status_code == 200:\n",
    "            results = response.json()['results'] # Set results as JSON response\n",
    "            text = results[0]['text'] # inside results, look in first group for section labeled 'text'\n",
    "            response_text = text.split('\\n')[0].replace(\"  \", \" \") # Optional, keep only the text before a new line, and replace double spaces with normal ones\n",
    "            conversation_history.append(f\"{fullmsg}{response_text}\\n\") # Add the response to the end of your conversation history\n",
    "        else:\n",
    "            print(response)\n",
    "        print(f\"{bot} {response_text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "a6001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
